{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c0b338",
   "metadata": {},
   "source": [
    "# ADS Thesis Adaptive PINNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a2f47c",
   "metadata": {},
   "source": [
    "## Import Libraries, PINN and Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62037217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.linalg import solve\n",
    "from scipy.stats import norm\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5157ad4",
   "metadata": {},
   "source": [
    "#### Define Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14bba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------------------------\n",
    "# Case 1: Single cosine wave\n",
    "# -------------------------\n",
    "def gen_testdata_1(n=1000):\n",
    "    x = np.linspace(0, 1, n)[:, None]\n",
    "    u = 10 * np.cos(np.pi * (x - 0.5) / 2)\n",
    "    return x, u\n",
    "\n",
    "def gen_bc_1(n=200):\n",
    "    x = np.vstack([np.zeros((n // 2, 1)), np.ones((n - n // 2, 1))])\n",
    "    u = 10 * np.cos(np.pi * (x - 0.5) / 2)\n",
    "    return x, u\n",
    "\n",
    "def pde_residual_1(model, x_tensor):\n",
    "    x = x_tensor.clone().detach().requires_grad_(True)\n",
    "    u = model(x)\n",
    "    u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "    rhs = -5 * np.pi * torch.sin(np.pi * (x - 0.5) / 2)\n",
    "    return u_x - rhs\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Case 2: Steep V-shape (smooth)\n",
    "# -------------------------\n",
    "def gen_testdata_2(n=1000):\n",
    "    k, x0 = 200.0, 0.5\n",
    "    x = np.linspace(0, 1, n)[:, None]\n",
    "    C0 = np.log(np.cosh(-k * x0)) / k\n",
    "    u = (np.log(np.cosh(k * (x - x0))) / k) - C0\n",
    "    return x, u\n",
    "\n",
    "def gen_bc_2(n=200):\n",
    "    k, x0 = 200.0, 0.5\n",
    "    x = np.vstack([np.zeros((n // 2, 1)), np.ones((n - n // 2, 1))])\n",
    "    C0 = np.log(np.cosh(-k * x0)) / k\n",
    "    u = (np.log(np.cosh(k * (x - x0))) / k) - C0\n",
    "    return x, u\n",
    "\n",
    "def pde_residual_2(model, x_tensor):\n",
    "    k, x0 = 200.0, 0.5\n",
    "    x = x_tensor.clone().detach().requires_grad_(True)\n",
    "    u = model(x)\n",
    "    u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "    rhs = torch.tanh(k * (x - x0))\n",
    "    return u_x - rhs\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Case 3: Steep tanh ramp\n",
    "# -------------------------\n",
    "def gen_testdata_3(n=1000):\n",
    "    k, x0 = 150.0, 0.6\n",
    "    x = np.linspace(0, 1, n)[:, None]\n",
    "    C0 = 0.5 * (1 + np.tanh(-k * x0))\n",
    "    u = 0.5 * (1 + np.tanh(k * (x - x0))) - C0\n",
    "    return x, u\n",
    "\n",
    "def gen_bc_3(n=200):\n",
    "    k, x0 = 150.0, 0.6\n",
    "    x = np.vstack([np.zeros((n // 2, 1)), np.ones((n - n // 2, 1))])\n",
    "    C0 = 0.5 * (1 + np.tanh(-k * x0))\n",
    "    u = 0.5 * (1 + np.tanh(k * (x - x0))) - C0\n",
    "    return x, u\n",
    "\n",
    "def pde_residual_3(model, x_tensor):\n",
    "    k, x0 = 150.0, 0.6\n",
    "    x = x_tensor.clone().detach().requires_grad_(True)\n",
    "    u = model(x)\n",
    "    u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "    rhs = 0.5 * k * (1 / torch.cosh(k * (x - x0))**2)\n",
    "    return u_x - rhs\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Case 4: Exponential increase\n",
    "# -------------------------\n",
    "def gen_testdata_4(n=1000):\n",
    "    u0, u1, k = 0.0, 5.0, 10.0\n",
    "    x = np.linspace(0, 1, n)[:, None]\n",
    "    u = u0 + (u1 - u0) * (1 - np.exp(k * x)) / (1 - np.exp(k))\n",
    "    return x, u\n",
    "\n",
    "def gen_bc_4(n=200):\n",
    "    u0, u1, k = 0.0, 5.0, 10.0\n",
    "    x = np.vstack([np.zeros((n // 2, 1)), np.ones((n - n // 2, 1))])\n",
    "    u = u0 + (u1 - u0) * (1 - np.exp(k * x)) / (1 - np.exp(k))\n",
    "    return x, u\n",
    "\n",
    "def pde_residual_4(model, x_tensor):\n",
    "    u0, u1, k = 0.0, 5.0, 10.0\n",
    "    L = 1.0\n",
    "    x = x_tensor.clone().detach().requires_grad_(True)\n",
    "    u = model(x)\n",
    "    u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "    rhs = (u1 - u0) * (-k * torch.exp(k * x)) / (1 - torch.exp(torch.tensor(k * L)))\n",
    "    return u_x - rhs\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Case 5: Multi-frequency decaying cosine\n",
    "# -------------------------\n",
    "def gen_testdata_5(n=1000):\n",
    "    x = np.linspace(0, 1, n)[:, None]\n",
    "    u = np.exp(-x) * np.cos(5 * np.pi * (x - 0.5))\n",
    "    return x, u\n",
    "\n",
    "def gen_bc_5(n=200):\n",
    "    x = np.vstack([np.zeros((n // 2, 1)), np.ones((n - n // 2, 1))])\n",
    "    u = np.exp(-x) * np.cos(5 * np.pi * (x - 0.5))\n",
    "    return x, u\n",
    "\n",
    "def pde_residual_5(model, x_tensor):\n",
    "    x = x_tensor.clone().detach().requires_grad_(True)\n",
    "    u = model(x)\n",
    "    u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "    expm = torch.exp(-x)\n",
    "    arg = 5 * np.pi * (x - 0.5)\n",
    "    rhs = -expm * torch.cos(arg) + expm * 5 * np.pi * torch.sin(arg)\n",
    "    return u_x - rhs\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Case 6: Piecewise constant (discontinuous)\n",
    "# -------------------------\n",
    "def gen_testdata_6(n=1000):\n",
    "    x = np.linspace(0, 1, n)[:, None]\n",
    "    u = np.where(x < 0.5, 10.0, 1.0)\n",
    "    return x, u\n",
    "\n",
    "def gen_bc_6(n=200):\n",
    "    x = np.vstack([np.zeros((n // 2, 1)), np.ones((n - n // 2, 1))])\n",
    "    u = np.where(x < 0.5, 10.0, 1.0)\n",
    "    return x, u\n",
    "\n",
    "def pde_residual_6(model, x_tensor):\n",
    "    x = x_tensor.clone().detach().requires_grad_(True)\n",
    "    u = model(x)\n",
    "    u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "    return u_x\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Case Dictionary\n",
    "# -------------------------\n",
    "ode_cases = {\n",
    "    1: (gen_testdata_1, gen_bc_1, pde_residual_1),\n",
    "    2: (gen_testdata_2, gen_bc_2, pde_residual_2),\n",
    "    3: (gen_testdata_3, gen_bc_3, pde_residual_3),\n",
    "    4: (gen_testdata_4, gen_bc_4, pde_residual_4),\n",
    "    5: (gen_testdata_5, gen_bc_5, pde_residual_5),\n",
    "    6: (gen_testdata_6, gen_bc_6, pde_residual_6)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1b98e",
   "metadata": {},
   "source": [
    "#### Define Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0938151",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_pde_bc_loss (pde_hist, bc_hist):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(pde_hist, label='PDE Loss', color='blue')\n",
    "    plt.plot(bc_hist, label='BC Loss', color='orange')\n",
    "    plt.yscale('log')\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.xlabel('Epochs', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.grid(ls='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_comparison(results, metric=\"loss\", case_name=\"Case 3\"):\n",
    "    \"\"\"\n",
    "    results: list of dicts with keys:\n",
    "      - 'name': str\n",
    "      - 'include_boundaries': bool\n",
    "      - 'adaptive': bool\n",
    "      - 'loss_hist': list\n",
    "      - 'pde_loss_hist': list\n",
    "      - 'bc_loss_hist': list\n",
    "      - 'l2_hist': list\n",
    "    metric: \"loss\", \"pde\", \"bc\", or \"l2\"\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for result in results:\n",
    "        if metric == \"loss\":\n",
    "            y = result['loss_hist']\n",
    "            label = f\"{result['name']} | BC {'✓' if result['include_boundaries'] else '✗'}\"\n",
    "        elif metric == \"pde\":\n",
    "            y = result['pde_loss_hist']\n",
    "            label = f\"{result['name']} | PDE loss | BC {'✓' if result['include_boundaries'] else '✗'}\"\n",
    "        elif metric == \"bc\":\n",
    "            y = result['bc_loss_hist']\n",
    "            label = f\"{result['name']} | BC loss | BC {'✓' if result['include_boundaries'] else '✗'}\"\n",
    "        elif metric == \"l2\":\n",
    "            y = result['l2_hist']\n",
    "            label = f\"{result['name']} | L2 error | BC {'✓' if result['include_boundaries'] else '✗'}\"\n",
    "        else:\n",
    "            continue\n",
    "        plt.plot(y, label=label)\n",
    "    \n",
    "    titles = {\n",
    "        \"loss\": \"Total Loss (PDE + BC)\",\n",
    "        \"pde\": \"PDE Residual Loss\",\n",
    "        \"bc\": \"Boundary Condition Loss\",\n",
    "        \"l2\": \"L2 Relative Error on Test Set\"\n",
    "    }\n",
    "    plt.title(f\"{titles.get(metric, 'Loss')} | {case_name}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_solution_snapshots_compare(\n",
    "    x_test: np.ndarray,\n",
    "    u_test: np.ndarray,\n",
    "    pred_hist_adapt: list[np.ndarray],\n",
    "    pred_hist_nonadapt: list[np.ndarray],\n",
    "    ep_hist_pred: list[int],\n",
    "    n_cols: int = 4\n",
    "):\n",
    "    \"\"\"\n",
    "    Grid of solution snapshots comparing adaptive vs non-adaptive PINN.\n",
    "\n",
    "    Each subplot at epoch ep_hist_pred[i] shows:\n",
    "      • True u(x)      (black solid)\n",
    "      • Adaptive û(x) (blue dashed)\n",
    "      • Non‐adaptive û(x) (red dash‐dot)\n",
    "\n",
    "    Legend is placed in 3 columns above the grid.\n",
    "    \"\"\"\n",
    "    import math\n",
    "    x = x_test.flatten()\n",
    "    y_true = u_test.flatten()\n",
    "\n",
    "    # ensure we only loop over the common length\n",
    "    n_snap = min(len(pred_hist_adapt),\n",
    "                 len(pred_hist_nonadapt),\n",
    "                 len(ep_hist_pred))\n",
    "    if n_snap == 0:\n",
    "        raise ValueError(\"No snapshots to plot!\")\n",
    "\n",
    "    # set up grid\n",
    "    n_rows = math.ceil(n_snap / n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols,\n",
    "                             figsize=(4*n_cols, 3*n_rows),\n",
    "                             squeeze=False)\n",
    "\n",
    "    # plot each panel\n",
    "    for idx in range(n_snap):\n",
    "        ax = axes[idx//n_cols][idx % n_cols]\n",
    "        ep = ep_hist_pred[idx]\n",
    "        y_ad = pred_hist_adapt[idx].flatten()\n",
    "        y_na = pred_hist_nonadapt[idx].flatten()\n",
    "\n",
    "        # only label on the first panel; we'll pull these handles for the global legend\n",
    "        if idx == 0:\n",
    "            ax.plot(x, y_true, 'k-',  lw=1.5, label='True')\n",
    "            ax.plot(x, y_ad,   'b--', lw=1.5, label='Adaptive')\n",
    "            ax.plot(x, y_na,  'r-.', lw=1.5, label='Non-adaptive')\n",
    "        else:\n",
    "            ax.plot(x, y_true, 'k-',  lw=1.5)\n",
    "            ax.plot(x, y_ad,   'b--', lw=1.5)\n",
    "            ax.plot(x, y_na,  'r-.', lw=1.5)\n",
    "\n",
    "        ax.set_title(f\"Epoch {ep}\")\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('u(x)')\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    # delete any extra axes\n",
    "    for j in range(n_snap, n_rows*n_cols):\n",
    "        fig.delaxes(axes[j//n_cols][j % n_cols])\n",
    "\n",
    "    # grab handles from the first axis\n",
    "    handles, labels = axes[0][0].get_legend_handles_labels()\n",
    "\n",
    "    # place a 3-column legend above all subplots\n",
    "    fig.legend(\n",
    "        handles, labels,\n",
    "        loc='lower center',\n",
    "        bbox_to_anchor=(0.5, 0.98),\n",
    "        ncol=3,\n",
    "        frameon=False,\n",
    "        fontsize=14\n",
    "    )\n",
    "    # leave a bit less room at top for the legend\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_continuous_collocation_error_evolution(\n",
    "    pos_hist: list[np.ndarray],\n",
    "    ep_hist: list[int],\n",
    "    pred_hist: list[np.ndarray],\n",
    "    res_hist: list[np.ndarray],\n",
    "    u_test_fn: callable  # function that gives u_true(x)\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a continuous heatmap of the evolution of two error metrics over a common spatial grid.\n",
    "    \n",
    "    For each epoch, the collocation error and PDE residual (both taken at the collocation nodes)\n",
    "    are interpolated onto a common x-grid and then plotted as two heatmaps:\n",
    "      - Top: absolute PDE residual error.\n",
    "      - Bottom: solution prediction error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pos_hist : list of np.ndarray\n",
    "        Node positions at each epoch.\n",
    "    ep_hist : list of int\n",
    "        Epoch numbers corresponding to pos_hist.\n",
    "    pred_hist : list of np.ndarray\n",
    "        Model predictions at the collocation nodes per epoch.\n",
    "    res_hist : list of np.ndarray\n",
    "        PDE residuals at the collocation nodes per epoch.\n",
    "    u_test_fn : callable\n",
    "        Function returning u_true(x) for a given array x.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Combine all positions to get a global common grid\n",
    "    all_x = np.concatenate([p.flatten() for p in pos_hist])\n",
    "    x_min, x_max = all_x.min(), all_x.max()    \n",
    "    x_grid = np.linspace(x_min, x_max, 500)\n",
    "\n",
    "    # Prepare arrays for the two error metrics, one row per epoch.\n",
    "    residual_field = []\n",
    "    sol_error_field = []\n",
    "    \n",
    "    for p, pred, res in zip(pos_hist, pred_hist, res_hist):\n",
    "        x_i = p.flatten()\n",
    "        # Interpolate PDE residual (absolute value) onto common grid.\n",
    "        r_i = np.abs(res.flatten())\n",
    "        r_interp = np.interp(x_grid, x_i, r_i, left=np.nan, right=np.nan)\n",
    "        residual_field.append(r_interp)\n",
    "        \n",
    "        # Compute solution error: compare prediction at collocation nodes with u_true.\n",
    "        # Here we interpolate the prediction onto the common grid.\n",
    "        u_pred_i = np.interp(x_grid, x_i, pred.flatten(), left=np.nan, right=np.nan)\n",
    "        u_true_grid = u_test_fn(x_grid)\n",
    "        sol_err = np.abs(u_pred_i - u_true_grid)\n",
    "        sol_error_field.append(sol_err)\n",
    "    \n",
    "    residual_field = np.array(residual_field)\n",
    "    sol_error_field = np.array(sol_error_field)\n",
    "    epochs = np.array(ep_hist)\n",
    "    \n",
    "    # Plot the continuous heatmaps.\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 10), sharex=True)\n",
    "    \n",
    "    # Plot residual field (top)\n",
    "    im1 = ax1.imshow(\n",
    "        residual_field,\n",
    "        aspect='auto',\n",
    "        extent=[x_min, x_max, epochs[-1], epochs[0]],\n",
    "        cmap='magma'\n",
    "    )\n",
    "    ax1.set_title(\"|PDE residual| evolution\")\n",
    "    ax1.set_ylabel(\"Epoch\")\n",
    "    fig.colorbar(im1, ax=ax1, label=\"|r|\")\n",
    "    \n",
    "    # Plot solution error field (bottom)\n",
    "    im2 = ax2.imshow(\n",
    "        sol_error_field,\n",
    "        aspect='auto',\n",
    "        extent=[x_min, x_max, epochs[-1], epochs[0]],\n",
    "        cmap='magma'\n",
    "    )\n",
    "    ax2.set_title(\"Solution error evolution\")\n",
    "    ax2.set_xlabel(\"x\")\n",
    "    ax2.set_ylabel(\"Epoch\")\n",
    "    fig.colorbar(im2, ax=ax2, label=\"|u_pred - u_true|\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_discrete_collocation_error_evolution(\n",
    "    pos_hist: list[np.ndarray],\n",
    "    ep_hist: list[int],\n",
    "    pred_hist: list[np.ndarray],\n",
    "    res_hist: list[np.ndarray],\n",
    "    u_test_fn: callable  # function that gives u_true(x)\n",
    "):\n",
    "    \"\"\"\n",
    "    Discrete scatter-heatmap of collocation node positions over training epochs,\n",
    "    colored by (1) |PDE residual| and (2) |solution error|.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pos_hist : list of np.ndarray\n",
    "        Node positions at each epoch (shape varies).\n",
    "    ep_hist : list of int\n",
    "        Epochs corresponding to pos_hist.\n",
    "    pred_hist : list of np.ndarray\n",
    "        Predictions at pos_hist per epoch.\n",
    "    res_hist : list of np.ndarray\n",
    "        Residuals at pos_hist per epoch.\n",
    "    u_test_fn : callable\n",
    "        Function that returns u_true(x) for arbitrary x.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 10))\n",
    "\n",
    "    for x_i, ep, u_pred_i, r_i in zip(pos_hist, ep_hist, pred_hist, res_hist):\n",
    "        x_i = x_i.flatten()\n",
    "        u_true_i = u_test_fn(x_i)\n",
    "        err_i = np.abs(u_pred_i.flatten() - u_true_i)\n",
    "        r_abs = np.abs(r_i.flatten())\n",
    "\n",
    "        # Scatter plots (epoch on y-axis, x on x-axis, color by value)\n",
    "        ax1.scatter(x_i, np.full_like(x_i, ep), c=r_abs, s=20, cmap='magma', alpha=0.8)\n",
    "        ax2.scatter(x_i, np.full_like(x_i, ep), c=err_i, s=20, cmap='magma', alpha=0.8)\n",
    "\n",
    "    for ax, title, label in zip(\n",
    "        (ax1, ax2),\n",
    "        (\"|PDE residual|\", \"Solution error\"),\n",
    "        (\"|r|\", \"|uₚᵣₑd − u_true|\")\n",
    "    ):\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('Epoch')\n",
    "        ax.set_title(title)\n",
    "        ax.grid(ls='--', alpha=0.3)\n",
    "\n",
    "    fig.colorbar(ax1.collections[0], ax=ax1, label='|r|')\n",
    "    fig.colorbar(ax2.collections[0], ax=ax2, label='|uₚᵣₑd − u_true|')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_collocation_error_evolution(\n",
    "    pos_hist, ep_hist,\n",
    "    model,\n",
    "    pde_residual_fn,\n",
    "    x_test, u_test,\n",
    "    device='cpu'\n",
    "):\n",
    "    \"\"\"\n",
    "    Scatter‐heatmap of collocation nodes over epochs,\n",
    "    colored by (1) |PDE residual| and (2) |solution error|.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pos_hist : list of 1D np.ndarray\n",
    "        Node‐positions at each recorded epoch.\n",
    "    ep_hist : list of ints\n",
    "        Corresponding epochs.\n",
    "    model : nn.Module\n",
    "        Your trained PINN (should be in eval() mode).\n",
    "    pde_residual_fn : callable\n",
    "        Residual function: res = pde_residual_fn(model, x_tensor).\n",
    "    x_test, u_test : np.ndarray\n",
    "        Dense test grid and true solution on it.\n",
    "    device : str\n",
    "        'cpu' or 'cuda'.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # build interpolant for analytic u\n",
    "    interp_u = interp1d(x_test.flatten(), u_test.flatten(), kind='cubic',\n",
    "                        fill_value='extrapolate')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18,10))\n",
    "    \n",
    "    for pts, epoch in zip(pos_hist, ep_hist):\n",
    "        # prepare tensor for residual\n",
    "        x_dom = pts[:,None]\n",
    "        x_t = torch.tensor(x_dom, dtype=torch.float32, device=device, requires_grad=True)\n",
    "        # compute abs‐residual\n",
    "        with torch.enable_grad():\n",
    "            r = pde_residual_fn(model, x_t).abs().detach().cpu().numpy().flatten()\n",
    "        # compute solution error\n",
    "        with torch.no_grad():\n",
    "            u_pred = model(torch.tensor(x_dom, dtype=torch.float32, device=device)).cpu().numpy().flatten()\n",
    "        u_true = interp_u(pts)\n",
    "        sol_err = np.abs(u_pred - u_true)\n",
    "        \n",
    "        # scatter for residual\n",
    "        sc1 = ax1.scatter(pts, np.full_like(pts, epoch),\n",
    "                          c=r, s=20, cmap='magma', alpha=0.8)\n",
    "        # scatter for sol‐error\n",
    "        sc2 = ax2.scatter(pts, np.full_like(pts, epoch),\n",
    "                          c=sol_err, s=20, cmap='magma', alpha=0.8)\n",
    "    \n",
    "    # labels & colorbars\n",
    "    for ax, title in zip((ax1, ax2), (\"|PDE residual|\", \"Solution error\")):\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('Epoch')\n",
    "        ax.set_title(title)\n",
    "        ax.grid(ls='--', alpha=0.3)\n",
    "    fig.colorbar(sc1, ax=ax1, label='|r|')\n",
    "    fig.colorbar(sc2, ax=ax2, label='|uₚᵣₑd–u_true|')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Plotting utils (unchanged) ---\n",
    "def plot_training_and_solution(loss_hist, l2_hist, t_test, u_test, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        u_pred = model(torch.tensor(t_test, dtype=torch.float32, device=device)).cpu().numpy()\n",
    "    plt.figure(figsize=(18,6))\n",
    "    ax1 = plt.subplot(1,3,1)\n",
    "    ax1.plot(loss_hist); ax1.set_yscale('log'); ax1.set_title('Training Loss', fontsize=14)\n",
    "    ax2 = plt.subplot(1,3,2)\n",
    "    ax2.plot(l2_hist); ax2.set_yscale('log'); ax2.set_title('Test Loss', fontsize=14)\n",
    "    ax3 = plt.subplot(1,3,3)\n",
    "    ax3.plot(t_test,u_test,'k-',label='True')\n",
    "    ax3.plot(t_test,u_pred,'r--',label='Pred'); ax3.set_title('Solution')\n",
    "    ax3.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_test_error(l2_hist_ad, l2_hist_na, epochs_ad, epochs_na):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epochs_ad, l2_hist_ad, 'b-', label='Adaptive PINN')\n",
    "    plt.plot(epochs_na, l2_hist_na, 'r--', label='Non-adaptive PINN')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.ylabel('L2 Error', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(ls='--', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_collocation_evolution(pos_hist, ep_hist):\n",
    "    plt.figure(figsize=(18,6))\n",
    "    for pts,epoch in zip(pos_hist,ep_hist):\n",
    "        plt.scatter(pts, np.full_like(pts,epoch),s=5,alpha=0.6)\n",
    "    plt.xlabel('x', fontsize=14); plt.ylabel('Epoch', fontsize=14); plt.title('Collocation Evolution',  fontsize=16)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "def plot_adaptation_density(\n",
    "    model, x_dom_before, x_dom_after,\n",
    "    pde_residual_fn, device='cpu',\n",
    "    smoothing_window=5, density_bins=30, epoch=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize PDE residual and collocation-point densities before/after adaptation.\n",
    "\n",
    "    - Smoothed line of |PDE residual| over pre-adaptation points.\n",
    "    - Filled density plots for x_dom_before (black) and x_dom_after (red).\n",
    "\n",
    "    Parameters:\n",
    "    - model: PINN (in eval mode).\n",
    "    - x_dom_before: np.ndarray (n_dom,1)\n",
    "    - x_dom_after:  np.ndarray (n_dom,1)\n",
    "    - pde_residual_fn: function(model, x_tensor) -> residual tensor\n",
    "    - device: 'cpu' or 'cuda'\n",
    "    - smoothing_window: int for moving-average smoothing\n",
    "    - density_bins: int number of histogram bins\n",
    "    \"\"\"\n",
    "    # Prepare and sort\n",
    "    x = x_dom_before.flatten()\n",
    "    idx = np.argsort(x)\n",
    "    x_sorted = x[idx]\n",
    "\n",
    "    # Compute residual\n",
    "    x_t = torch.tensor(x_sorted[:, None], dtype=torch.float32, device=device).requires_grad_(True)\n",
    "    r_t = pde_residual_fn(model, x_t).abs()       # no torch.no_grad here\n",
    "    r = r_t.detach().cpu().numpy().flatten()\n",
    "\n",
    "    # Smooth\n",
    "    if smoothing_window > 1:\n",
    "        kern = np.ones(smoothing_window) / smoothing_window\n",
    "        r = np.convolve(r, kern, mode='same')\n",
    "\n",
    "    # Densities\n",
    "    bins = np.linspace(0.0, 1.0, density_bins+1)\n",
    "    d_before, _ = np.histogram(x_dom_before.flatten(), bins=bins, density=True)\n",
    "    d_after,  _ = np.histogram(x_dom_after.flatten(),  bins=bins, density=True)\n",
    "    centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "    # Plot\n",
    "    fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "    ax1.plot(x_sorted, r, color='black', lw=2, label='Smoothed |PDE residual|')\n",
    "    ax1.set_xlabel('x'); ax1.set_ylabel('Residual', color='black')\n",
    "    ax1.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.fill_between(centers, d_before, color='black', alpha=0.3, label='Density before')\n",
    "    ax2.fill_between(centers, d_after,  color='red',   alpha=0.3, label='Density after')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.set_ylim(0.25, None)\n",
    "    ax2.tick_params(axis='y', color='tab:gray')\n",
    "\n",
    "    # Combined legend\n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "    h2, l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1+h2, l1+l2, loc='upper left')\n",
    "\n",
    "    ax1.grid(ls='--', alpha=0.3)\n",
    "    plt.title(f\"Adaptive Node Movement at epoch: {epoch}\")\n",
    "    plt.tight_layout()\n",
    "    # save plot\n",
    "    plt.savefig(f\"plots/node_movement_at-{epoch}.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_spatial_loss_evolution(pde_grid, l2_grid, x_eval_grid, ep_hist):\n",
    "    pde_arr = np.array(pde_grid)\n",
    "    l2_arr = np.array(l2_grid)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 10), sharex=True)\n",
    "\n",
    "    im1 = ax1.imshow(\n",
    "        np.flipud(pde_arr),\n",
    "        aspect='auto',\n",
    "        extent=[x_eval_grid.min(), x_eval_grid.max(), ep_hist[0], ep_hist[-1]],\n",
    "        cmap='magma',\n",
    "        # norm=plt.matplotlib.colors.LogNorm(vmin=1e-40, vmax=1e1)\n",
    "        norm=plt.matplotlib.colors.LogNorm()\n",
    "\n",
    "    )\n",
    "    ax1.set_title(\"PDE Residual Loss |r(x)|²\", fontsize=16)\n",
    "    ax1.set_ylabel(\"Epoch\", fontsize=14)\n",
    "    fig.colorbar(im1, ax=ax1, label=\"PDE Loss\")\n",
    "\n",
    "    im2 = ax2.imshow(\n",
    "        np.flipud(l2_arr),\n",
    "        aspect='auto',\n",
    "        extent=[x_eval_grid.min(), x_eval_grid.max(), ep_hist[0], ep_hist[-1]],\n",
    "        cmap='magma',\n",
    "        # norm=plt.matplotlib.colors.LogNorm(vmin=1e-40, vmax=1e1)\n",
    "        norm=plt.matplotlib.colors.LogNorm()\n",
    "    )\n",
    "    ax2.set_title(\"Test L2 Loss |u_pred(x) - u_true(x)|²\", fontsize=16)\n",
    "    ax2.set_xlabel(\"x\", fontsize=14)\n",
    "    ax2.set_ylabel(\"Epoch\", fontsize=14)\n",
    "    fig.colorbar(im2, ax=ax2, label=\"L2 Error\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f14ce",
   "metadata": {},
   "source": [
    "#### Define PINN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.act = torch.tanh\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(layers[i], layers[i+1])\n",
    "            for i in range(len(layers) - 1)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.act(layer(x))\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac06611",
   "metadata": {},
   "source": [
    "------------\n",
    "## Adaptive Node Moving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83706409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_moving_1d(\n",
    "    coordinate: np.ndarray,\n",
    "    estimated_error: np.ndarray,\n",
    "    boundary_nodes_id: np.ndarray,\n",
    "    ratio_nodal_distance: float = 5,\n",
    "    relaxation: float = 1.0,\n",
    "    stifness: float = 0.5\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reallocates 1D nodes using a spring model based on error estimates.\n",
    "\n",
    "    This function adjusts the positions of nodes along a 1D domain such that the spacing\n",
    "    between nodes becomes proportional to a given error estimate, using a spring model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    coordinate : np.ndarray\n",
    "        Array of node coordinates along the 1D domain.\n",
    "\n",
    "    estimated_error : np.ndarray\n",
    "        Estimated error values used to redistribute the nodes. Higher errors lead to denser spacing.\n",
    "\n",
    "    boundary_nodes_id : np.ndarray\n",
    "        Indices of boundary nodes whose positions remain fixed during the redistribution.\n",
    "\n",
    "    ratio_nodal_distance : float, optional\n",
    "        Desired maximum ratio of the largest to the smallest nodal spacing, by default 5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    recollocated_coordinate : np.ndarray\n",
    "        The reallocated node coordinates after applying the node-moving algorithm.\n",
    "    \"\"\"\n",
    "    # eps = 1e-4\n",
    "    # estimated_error = np.clip(estimated_error, eps, None)\n",
    "\n",
    "    b = np.log(ratio_nodal_distance) / np.log(\n",
    "        np.max(estimated_error) / np.min(estimated_error)\n",
    "    )\n",
    "    # Adjust gradients\n",
    "    adjusted_error_estimate = estimated_error**b\n",
    "    # Calculate ke values\n",
    "    ke = stifness * (adjusted_error_estimate[:-1] + adjusted_error_estimate[1:])\n",
    "\n",
    "    num_nodes = len(coordinate)\n",
    "    inside_domain_node_id = [i for i in range(num_nodes) if i not in boundary_nodes_id]\n",
    "\n",
    "    # Create the K matrix using the calculated ke\n",
    "    k1 = np.diag(np.hstack([0, ke])) + np.diag(np.hstack([ke, 0]))\n",
    "    k2 = -np.diag(ke, -1)\n",
    "    k3 = -np.diag(ke, 1)\n",
    "    k = k1 + k2 + k3\n",
    "\n",
    "    reg = 1e-6\n",
    "    k += np.eye(k.shape[0]) * reg\n",
    "\n",
    "    f = np.zeros(len(coordinate))\n",
    "\n",
    "    # Modify K to impose boundary conditions\n",
    "    for i in boundary_nodes_id:\n",
    "        f = f - np.dot(\n",
    "            k[:, i],\n",
    "            coordinate[i],\n",
    "        )\n",
    "    k = np.delete(k, boundary_nodes_id, axis=0)\n",
    "    k = np.delete(k, boundary_nodes_id, axis=1)\n",
    "    f = np.delete(f, boundary_nodes_id)\n",
    "\n",
    "    recollocate_inside_coordinate = solve(k, f)\n",
    "\n",
    "    # Reconstruct the full coordinate array including fixed boundary nodes\n",
    "    recollocated_coordinate = coordinate.copy()\n",
    "    recollocated_coordinate[inside_domain_node_id] = recollocate_inside_coordinate\n",
    "\n",
    "    if relaxation > 0.0:\n",
    "        recollocated_coordinate = (\n",
    "            coordinate + relaxation * (recollocated_coordinate - coordinate)\n",
    "        )\n",
    "\n",
    "    return recollocated_coordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41677e",
   "metadata": {},
   "source": [
    "### (Extra) Random Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a44d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_resampling_1d(\n",
    "    coordinate: np.ndarray,\n",
    "    boundary_nodes_id: np.ndarray,\n",
    "    **kwargs  # absorb other arguments like ratio_nodal_distance, stifness\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Randomly resamples all non-boundary nodes within [x_min, x_max].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coordinate : np.ndarray\n",
    "        Current 1D node coordinates.\n",
    "\n",
    "    boundary_nodes_id : np.ndarray\n",
    "        Indices of fixed boundary nodes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        New coordinates array with random interior resampling.\n",
    "    \"\"\"\n",
    "    num_nodes = len(coordinate)\n",
    "    x_min, x_max = coordinate.min(), coordinate.max()\n",
    "\n",
    "    interior_ids = [i for i in range(num_nodes) if i not in boundary_nodes_id]\n",
    "    new_coords = coordinate.copy()\n",
    "\n",
    "    # Random uniform resampling for interior nodes\n",
    "    resampled = np.random.uniform(x_min, x_max, size=len(interior_ids))\n",
    "\n",
    "    new_coords[interior_ids] = resampled\n",
    "    return new_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4da3f1d",
   "metadata": {},
   "source": [
    "### The Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98889088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer,\n",
    "          gen_testdata, gen_bc, pde_residual,\n",
    "          n_dom=200, n_bc=200, n_test=1000,\n",
    "          epochs=1000, refine_every=None, relaxation=1.0, include_boundaries=False, adaptive_fn=None, stifness=0.5):\n",
    "\n",
    "    # generate domain data, now includes boundaries 0 and 1\n",
    "    x_dom = np.random.uniform(0, 1, (n_dom, 1))\n",
    "    if include_boundaries:\n",
    "        x_dom = np.vstack(([[0.0]], x_dom, [[1.0]]))\n",
    "\n",
    "    x_bc, u_bc = gen_bc(n_bc)\n",
    "    x_test, u_test = gen_testdata(n_test)\n",
    "\n",
    "    x_bc_t  = torch.tensor(x_bc, dtype=torch.float32, device=device)\n",
    "    u_bc_t  = torch.tensor(u_bc, dtype=torch.float32, device=device)\n",
    "\n",
    "    loss_hist, l2_hist = [], []\n",
    "    pred_hist, ep_hist_pred, res_hist = [], [], []\n",
    "    pos_hist, ep_hist = [x_dom.flatten().copy()], []\n",
    "    pde_loss_hist, bc_loss_hist = [], []\n",
    "\n",
    "    pde_grid = []\n",
    "    l2_grid = []\n",
    "    u_test_fn = interp1d(x_test.flatten(), u_test.flatten(), kind='cubic', fill_value='extrapolate')\n",
    "    x_eval_grid = None\n",
    "\n",
    "    start = time.time()\n",
    "    for ep in range(1, epochs+1):\n",
    "        epochs_to_threshold = ep\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_dom_t = torch.tensor(x_dom, dtype=torch.float32, device=device)\n",
    "        r = pde_residual(model, x_dom_t)\n",
    "        loss_pde = (r**2).mean()\n",
    "\n",
    "        u_bc_pred = model(x_bc_t)\n",
    "        loss_bc   = ((u_bc_pred - u_bc_t)**2).mean()\n",
    "\n",
    "        pde_loss_hist.append(loss_pde.item())\n",
    "        bc_loss_hist.append(loss_bc.item())\n",
    "\n",
    "        loss = loss_pde + loss_bc\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_hist.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            u_pred = model(torch.tensor(x_test, dtype=torch.float32, device=device))\n",
    "            l2 = float(np.linalg.norm(u_pred.cpu().numpy() - u_test) / np.linalg.norm(u_test))\n",
    "        \n",
    "        if ep % 20 == 0:\n",
    "            l2_hist.append(l2)\n",
    "\n",
    "        # Evaluate losses over high-res grid\n",
    "        if ep % 1 == 0:\n",
    "            x_grid = np.linspace(0, 1, 500)[:, None]\n",
    "            x_grid_t = torch.tensor(x_grid, dtype=torch.float32, device=device, requires_grad=True)\n",
    "\n",
    "            # PDE residual at each x_grid point\n",
    "            with torch.enable_grad():\n",
    "                r_grid = pde_residual(model, x_grid_t).detach().cpu().numpy().flatten()\n",
    "                pde_grid.append(r_grid**2)\n",
    "\n",
    "            # Prediction and L2 error at each x_grid point\n",
    "            with torch.no_grad():\n",
    "                u_pred_grid = model(x_grid_t).cpu().numpy().flatten()\n",
    "                l2_grid.append((u_pred_grid - u_test_fn(x_grid.flatten()))**2)\n",
    "\n",
    "            x_eval_grid = x_grid.flatten()\n",
    "\n",
    "        stop_threshold = 0.001\n",
    "        if l2 < stop_threshold:\n",
    "            epochs_to_threshold = ep\n",
    "            break\n",
    "\n",
    "        # if ep % 1 == 0:\n",
    "        u_pred = model(torch.tensor(x_dom, dtype=torch.float32, device=device)).detach().numpy()\n",
    "        pred_hist.append(u_pred)\n",
    "        res_hist.append(r.detach().numpy())\n",
    "        ep_hist_pred.append(ep)\n",
    "        # print(f\"Epoch: {ep}, n_dom size: {len(x_dom)}, Loss: {loss.item():.4e}, L2 Error: {l2:.4e}\")\n",
    "\n",
    "        pos_hist.append(x_dom.flatten().copy())\n",
    "        ep_hist.append(ep)\n",
    "\n",
    "        if refine_every and ep % refine_every == 0:\n",
    "            global pde_residual_global\n",
    "            pde_residual_global = pde_residual\n",
    "\n",
    "            \n",
    "            coords = x_dom.flatten()\n",
    "            sort_idx = np.argsort(coords)\n",
    "            coords_sorted = coords[sort_idx]\n",
    "\n",
    "            x_full_t = torch.tensor(coords_sorted[:, None], dtype=torch.float32, device=device).requires_grad_(True)\n",
    "            r_full = pde_residual_global(model, x_full_t).detach().abs().cpu().numpy().flatten()\n",
    "\n",
    "            x_bc_small, u_bc_small = gen_bc(2)\n",
    "            u_bc_small = u_bc_small.flatten()\n",
    "            with torch.no_grad():\n",
    "                u_full_pred = model(x_full_t).cpu().numpy().flatten()\n",
    "\n",
    "            bc_err = np.zeros_like(r_full)\n",
    "            bc_err[0] = abs(u_full_pred[0] - u_bc_small[0])\n",
    "            bc_err[-1] = abs(u_full_pred[-1] - u_bc_small[1])\n",
    "\n",
    "            total_err = r_full + bc_err\n",
    "            boundary_ids = np.array([0, len(coords_sorted) - 1], dtype=int)\n",
    "\n",
    "            if adaptive_fn == node_moving_1d:\n",
    "                new_coords_full = node_moving_1d(\n",
    "                    coords_sorted,\n",
    "                    total_err,\n",
    "                    boundary_ids,\n",
    "                    ratio_nodal_distance=5,\n",
    "                    relaxation=relaxation,\n",
    "                    stifness=stifness \n",
    "                )\n",
    "                x_dom = new_coords_full.reshape(-1, 1)\n",
    "            elif adaptive_fn == random_resampling_1d:\n",
    "                new_coords_full = random_resampling_1d(\n",
    "                    coords_sorted,\n",
    "                    boundary_ids,\n",
    "                )\n",
    "                x_dom = new_coords_full.reshape(-1, 1)\n",
    "\n",
    "            # x_dom_before = x_dom.copy()\n",
    "            # x_dom_after  = new_coords_full.reshape(-1, 1).copy()\n",
    "            # plot_adaptation_density(model, x_dom_before, x_dom_after, pde_residual_global, device=device, epoch=ep)\n",
    "    end = time.time() - start\n",
    "\n",
    "    return loss_hist, l2_hist, pos_hist, ep_hist, ep_hist_pred, x_test, u_test, pred_hist, \\\n",
    "        epochs_to_threshold, end, pde_loss_hist, bc_loss_hist, res_hist, pde_grid, l2_grid, x_eval_grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f67b17",
   "metadata": {},
   "source": [
    "------------\n",
    "## Calling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ee88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "\n",
    "    # random sampling and node moving\n",
    "    repeats = 1\n",
    "    model_configs = []\n",
    "    cases = [1]\n",
    "    ndom_list = [200] \n",
    "    reallocate_every = [200]\n",
    "    for case in cases:\n",
    "        for n_dom in ndom_list:\n",
    "                for method in [None, node_moving_1d, random_resampling_1d]:\n",
    "                    method_name = method.__name__ if method is not None else \"None\"\n",
    "                    for refine in reallocate_every:\n",
    "                        model_configs.append({\n",
    "                            'case':             case,\n",
    "                            'name':             f\"Case{case}_nDom{n_dom}_refine{refine}_method{method_name}\",\n",
    "                            'n_dom':            n_dom,\n",
    "                            'refine':           refine,\n",
    "                            'adaptive_method':  method,\n",
    "                            'epochs':           10000,  \n",
    "                            'n_repeats':        repeats,\n",
    "                            'include_boundaries': False\n",
    "                        })\n",
    "\n",
    "    results_dir = 'results'\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "    # Write results to dataframe\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'case','name', 'method', 'NN', 'n_dom', 'n_bc', 'refine',\n",
    "        'l2_error','l2_hist','loss_hist','x_test','u_test','pred_hist', 'u_pred', 'time', \n",
    "        'epochs', 'pos_hist', 'include_boundaries','pde_loss_hist', 'bc_loss_hist', 'delay_adapt', 'stifness'\n",
    "    ])\n",
    "\n",
    "    print(f\"Running {len(model_configs)} models with {repeats} repeats each.\")\n",
    "    print(\"Results will be saved to:\", results_dir)\n",
    "    total = len(model_configs) * repeats\n",
    "    i, ave_time = 0, 0\n",
    "\n",
    "    for cfg in model_configs:\n",
    "        # Calculate waiting time based on average\n",
    "        waiting_time_mins = (ave_time / (i+1)) * (total - i) / 60\n",
    "        waiting_time_str = f\"{waiting_time_mins:.2f} mins\"\n",
    "        print(f\"\\n### {cfg['name']} \\t|\\t Wait ~ {waiting_time_str}###\")\n",
    "        gen_xd, gen_bc_f, pde_res = ode_cases[cfg['case']]\n",
    "\n",
    "        l2_average = []\n",
    "\n",
    "        for run in range(cfg['n_repeats']):\n",
    "            i += 1\n",
    "            # reseed for reproducibility\n",
    "            seed = 43 + run\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            # Define the PINN\n",
    "            if ('n_layers' in cfg) and ('n_neurons' in cfg):\n",
    "                model = PINN([1] + [cfg['n_neurons']]*(cfg['n_layers']) + [1]).to(device)\n",
    "            else:\n",
    "                model = PINN([1,20,20,1]).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "            # Call the training loop\n",
    "            loss_hist, l2_hist_ad, pos_hist, ep_hist_ad, ep_hist_pred,\\\n",
    "                x_test, u_test, pred_hist, epochs_to_threshold_ad, \\\n",
    "                    end_ad, pde_loss_hist, bc_loss_hist, res_hist, \\\n",
    "                        pde_grid, l2_grid, x_eval_grid = train(\n",
    "\n",
    "                model, optimizer,\n",
    "                gen_xd, gen_bc_f, pde_res,\n",
    "                n_dom=cfg['n_dom'] if 'n_dom' in cfg else 50, \n",
    "                n_bc=cfg['n_bc'] if 'n_bc' in cfg else 2, \n",
    "                n_test=cfg.get('n_test', 10000),\n",
    "                epochs=cfg.get('epochs', 10000),\n",
    "                refine_every=cfg['refine'],\n",
    "                adaptive_fn=cfg['adaptive_method'],\n",
    "                relaxation=cfg.get('relaxation', 1.0),\n",
    "                include_boundaries=cfg.get('boundary_in_xdom', False),\n",
    "                stifness = cfg.get('spring_stifness', 0.5),\n",
    "            )\n",
    "            ave_time += end_ad\n",
    "\n",
    "            pred_hist_ad = pred_hist\n",
    "            with torch.no_grad():\n",
    "                u_ad = model(torch.tensor(x_test, dtype=torch.float32, device=device)).cpu().numpy()\n",
    "\n",
    "            print(f\"Iteration: {i}/{total}, \\t|\\t Final L2 error: {l2_hist_ad[-1]:.3e}, \\t|\\t Time: {end_ad:.2f}s, \\t|\\t Epochs: {epochs_to_threshold_ad}\")\n",
    "\n",
    "\n",
    "            # # --- PLOTTING!! ---\n",
    "            # if cfg['refine'] != 0:\n",
    "            #     plot_collocation_evolution(pos_hist, ep_hist_ad)\n",
    "\n",
    "            # plot_pde_bc_loss(pde_loss_hist, bc_loss_hist)\n",
    "            # # # print(f\"PDE loss: {pde_loss_hist[-1]:.3e}, \\t|\\t BC loss: {bc_loss_hist[-1]:.3e}\")\n",
    "            # # plot_training_and_solution(\n",
    "            # #     loss_hist, l2_hist_ad, x_test, u_test, model\n",
    "            # # )\n",
    "\n",
    "            # plot_spatial_loss_evolution(pde_grid, l2_grid, x_eval_grid, ep_hist_pred)\n",
    "\n",
    "\n",
    "            # Store the results in the DataFrame\n",
    "            # ! Lists take up a lot of space so only store them when necessary\n",
    "            new_row = {\n",
    "                'case':    cfg['case'],\n",
    "                'name':    cfg['name'],\n",
    "                'method':  cfg.get('adaptive_method', 'PINN'),\n",
    "                'NN':      f\"{cfg['n_layers']}x{cfg['n_neurons']}\" if 'n_layers' in cfg else '2x20',\n",
    "                'n_dom':   cfg['n_dom'],\n",
    "                'refine':  cfg['refine'],\n",
    "                'l2_error': l2_hist_ad[-1],\n",
    "                # 'relaxation': cfg.get('relaxation', 1.0),\n",
    "                'l2_hist': l2_hist_ad, \n",
    "                # 'loss_hist': loss_hist,\n",
    "                # 'x_test':  x_test,\n",
    "                # 'u_test':  u_test,\n",
    "                # 'pred_hist':  pred_hist,\n",
    "                'time':    end_ad,\n",
    "                'epochs':  epochs_to_threshold_ad,\n",
    "                # 'pos_hist': pos_hist,\n",
    "                # 'include_boundaries': cfg.get('boundary_in_xdom', False),\n",
    "                'pde_loss_hist': pde_loss_hist[-1],\n",
    "                'bc_loss_hist': bc_loss_hist[-1],\n",
    "                # 'delay_adapt': cfg.get('delay_adapt', 0),\n",
    "                # 'stifness': cfg.get('spring_stifness', 0.5),\n",
    "            }\n",
    "            results_df.loc[len(results_df)] = new_row  \n",
    "            \n",
    "            l2_average.append(l2_hist_ad[-1])\n",
    "    \n",
    "        print(f\"\\nAverage L2 error across all runs: {np.mean(l2_average):.3e}\")\n",
    "        print(f\"Average time per run: {ave_time / i:.2f} seconds\")\n",
    "\n",
    "    # Save results to CSV or pickle\n",
    "    # results_df.to_csv(os.path.join(results_dir, 'results.csv'), index=False)\n",
    "    results_df.to_pickle(os.path.join(results_dir, 'results.pkl'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
